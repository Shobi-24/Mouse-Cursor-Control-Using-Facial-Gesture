This project lets you control your computer without using your hands instead, it responds to your eye blinks, facial movements, and head position, making it especially useful for people who struggle with traditional mouse input. The system tracks your face through a webcam using OpenCV and MediaPipe, measures eye openness for blink detection, mouth opening for drag actions, and nose movement for scrolling. Before use, it runs through a simple calibration where you keep your eyes open, blink, open your mouth widely, and hold a neutral face, so it learns your natural ranges and avoids random triggers. Once active, one blink equals a left click, two blinks give a right click, three blinks perform a double-click, a wide mouth starts dragging, and looking up or down scrolls the screen. Holding your eyebrows up safely exits the program, and you can pause or resume with a single key. The system works well when lighting is good and your face stays in frame, but it still depends on threshold values — not deep learning — so it’s not perfect at telling intentional actions from accidental ones. Still, it’s a practical starting point for accessible computing, and adding LSTM-based gesture recognition down the line would make it even smarter and more reliable.
